# -*- coding: utf-8 -*-
"""Group 4 New Dataset Project 2 12052024 1201PM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vVo1uHBsocBpA7hsNXQYd5KECogjA4yb

##1.1. Importing the basic libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# 1.1.1 Load data processing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math
# %matplotlib inline

"""##1.2. Importing machine learning libraries"""

# 1.2.1 Load machine learning libraries
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn.decomposition import PCA
from sklearn.neural_network import MLPClassifier
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder # Import the LabelEncoder class
from mpl_toolkits.mplot3d import Axes3D # Import the necessary module for 3D plotting

"""##1.3 Upload Dataset"""

# 1.3.1
import os

curr_dir = os.getcwd()
print(curr_dir)

# 1.3.2
from google.colab import files
uploaded = files.upload()

"""##1.4 Verify File(s)"""

# 1.4.1
!ls -al /content/

"""##1.5 Create Data Frame From Dataset"""

# 1.5.1
df=pd.read_csv("Netflow_data.csv") # 2-dimensional data structure (rows X columns) created from reading .csv and assigned name df
print(f'\n Dataset Shape: {df.shape}')
print(f'\n First 5 Rows: {df.head(5)}')
print(f'\n Last 5 Rows: {df.tail(5)}')

"""## 1.6 Review Data Frame"""

# 1.6.1
df.head(5) #print first 5 rows with header

# 1.62 Review Data Frame Structure and Content
df.info()

#1.63 Verify Empty (null) Fields
df.isnull().sum() #This method is applied to the DataFrame. It checks each cell in the DataFrame and returns a new DataFrame of the same shape, but with Boolean values (True or False). Missing values

#1.63 print column names and total columns
col_name_list = list(df.columns)
print(col_name_list)
col_size = len(col_name_list)
print(col_size)

"""##1.7 Data Preprocessing / Cleaning"""

# 1.7.1 Convert categorical data to numerical
labelencoder = LabelEncoder()
df['service'] = labelencoder.fit_transform(df['service'])

"""###1.71 Data Cleaning"""

# 1.7.2 Convert attack feature to numbers using one hot encoding

# Create a mapping for attack to numerical values
attack_mapping = {'benign': 0, 'attack': 1}

# Create a mapping for protocol_type to numerical values
protocol_type_mapping = {'icmp': 0, 'udp': 1, 'tcp': 2}

# Create a mapping for flag to numerical values
flag_mapping = {'SF': 0, 'S0': 1, 'REJ': 2, 'RSTR': 3, 'SH': 4, 'RSTO': 5, 'S1': 6, 'RSTOS0': 7, 'S3': 8,
       'S2': 9, 'OTH': 10}


# Apply one-hot encoding to the 'attack' column
df['protocol_type'] = df['protocol_type'].map(protocol_type_mapping)

# Apply one-hot encoding to the 'flag' column
df['flag'] = df['flag'].map(flag_mapping)

# Apply one-hot encoding to the 'attack' column
df['attack'] = df['attack'].map(attack_mapping)

df['attack'] = df['attack'].astype(int)

df.head(5)
# 1.7.3 print first 5 rows with header to verify changes

# 1.7.4 count the number of unique values in each column

for col in df.columns:
  print(f"Column '{col}': {df[col].nunique()} unique values")

# 1.7.5 verify column names
(df.columns)

# 1.7.6 drop features with low correlation or signifigance based on heatmap/correlation matrix, and other modules. Recurive and repeating process based on evolving results.
# create new df (features_df)
#features_df = df.drop(['dst_host_srv_rerror_rate', 'wrong_fragment', 'src_bytes', 'dst_bytes', 'is_host_login', 'land','num_outbound_cmds','dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_diff_host_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate','dst_host_srv_count','dst_host_same_srv_rate','diff_srv_rate','srv_diff_host_rate','dst_host_count','srv_serror_rate','rerror_rate', 'same_srv_rate'], axis=1, inplace=False)
features_df = df.drop(['dst_host_srv_rerror_rate', 'num_failed_logins', 'urgent', 'wrong_fragment', 'src_bytes', 'num_shells', 'root_shell',  'num_access_files',  'su_attempted','num_file_creations', 'dst_bytes', 'is_host_login', 'land','num_outbound_cmds','dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_diff_host_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate','dst_host_srv_count','dst_host_same_srv_rate','diff_srv_rate','srv_diff_host_rate','dst_host_count','srv_serror_rate','rerror_rate', 'same_srv_rate'], axis=1, inplace=False)

# 1.7.7 #verify new columns
features_df.columns

"""##1.8 Exploratory Data Analysis (EDA)"""

# 1.8.1 plot frequency use mapped names

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'features_df' is your DataFrame and 'attack' is the target column
# Calculate the frequency of each attack type
attack_counts = features_df['attack'].value_counts()

# Create a bar plot
plt.figure(figsize=(8, 6))
sns.barplot(x=attack_counts.index, y=attack_counts.values)
plt.title('Frequency of Attack Types')
plt.xlabel('Attack Type')
plt.ylabel('Frequency')

# Map numerical attack labels back to their original names (if needed)
attack_mapping_reverse = {v: k for k, v in attack_mapping.items()}  # Reverse the mapping
plt.xticks(ticks=attack_counts.index, labels=[attack_mapping_reverse[i] for i in attack_counts.index])


plt.show()

# 1.8.2 attacks type by protocol use reverse mapping

# Assuming 'features_df' is your DataFrame and 'protocol_type' is the column with protocol types
# and 'attack' is the target column

# Calculate the frequency of each attack type for each protocol
attack_by_protocol = features_df.groupby('protocol_type')['attack'].value_counts(normalize=True).unstack()

# Reverse mapping for protocol_type
protocol_type_mapping_reverse = {v: k for k, v in protocol_type_mapping.items()}

# Reverse mapping for attack type
attack_mapping_reverse = {v: k for k, v in attack_mapping.items()}

# Rename columns and index using the reverse mapping
attack_by_protocol = attack_by_protocol.rename(columns=attack_mapping_reverse, index=protocol_type_mapping_reverse)


# Create a bar plot
attack_by_protocol.plot(kind='bar', stacked=True, figsize=(10, 6))
plt.title('Attack Types by Protocol')
plt.xlabel('Protocol Type')
plt.ylabel('Proportion of Attacks')
plt.xticks(rotation=0)  # Rotate x-axis labels for better readability
plt.legend(title='Attack Type')
plt.show()

# 1.8.3 pie chart of protocol type reverse mapping labels

# Assuming 'features_df' is your DataFrame and 'protocol_type' is the column with protocol types
# and 'attack' is the target column

# Calculate the frequency of each attack type for each protocol
attack_by_protocol = features_df.groupby('protocol_type')['attack'].value_counts().unstack()

# Reverse mapping for protocol_type
protocol_type_mapping_reverse = {v: k for k, v in protocol_type_mapping.items()}

# Reverse mapping for attack type
attack_mapping_reverse = {v: k for k, v in attack_mapping.items()}

# Rename columns and index using the reverse mapping
attack_by_protocol = attack_by_protocol.rename(columns=attack_mapping_reverse, index=protocol_type_mapping_reverse)

# Create a pie chart for each protocol type
for protocol, data in attack_by_protocol.iterrows():
    plt.figure(figsize=(8, 6))
    plt.pie(data, labels=data.index, autopct='%1.1f%%', startangle=90)
    plt.title(f'Attack Distribution for Protocol: {protocol}')
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.show()

# 1.8.4 count by logged_in use labels

# Assuming 'features_df' is your DataFrame and 'attack' is the target column
attack_counts = features_df.groupby('logged_in')['attack'].value_counts()

# Create a bar plot
plt.figure(figsize=(8, 6))
attack_counts.unstack().plot(kind='bar', stacked=True)
plt.title('Attack Types by logged_in')
plt.xlabel('logged_in')
plt.ylabel('Frequency')

# Map numerical attack labels back to their original names
attack_mapping_reverse = {v: k for k, v in attack_mapping.items()}
plt.legend(title='Attack Type', labels=[attack_mapping_reverse[i] for i in attack_counts.index.get_level_values(1)])

plt.show()

# 1.8.5 correlation matrix

# Calculate the correlation matrix
# Select only the numerical columns for the correlation matrix
numerical_cols = features_df.select_dtypes(include=np.number)
correlation_matrix = numerical_cols.corr()


# Display the correlation matrix
print(correlation_matrix)

# You can also visualize the correlation matrix using a heatmap
plt.figure(figsize=(20, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Features')
plt.show()

# 1.8.6 histrogram
print("Histogram Analysis")
features_df.hist(bins=50, figsize=(15,10))
plt.show()

"""##1.9 Training"""

# 1.9.1 Define features (X) and target (y)
X = features_df.drop('attack', axis=1)
y = features_df['attack']

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 1.9.2 KNN

# Define features (X) and target (y)
#X = features_df.drop('attack', axis=1)
#y = features_df['attack']

# Split data into training and testing sets
#from sklearn.model_selection import train_test_split
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features using StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train a KNN classifier
knn_classifier = KNeighborsClassifier(n_neighbors=5) # You can adjust the number of neighbors
knn_classifier.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = knn_classifier.predict(X_test_scaled)

# Evaluate the model
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

# prompt: create confusion matrix

# Assuming y_test and y_pred are already defined from your KNN model
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Benign', 'Attack'],
            yticklabels=['Benign', 'Attack'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# 1.9.3 SVM

# Initialize and train an SVM classifier
svm_classifier = SVC(kernel='linear', C=1) # You can adjust the kernel and C parameter
svm_classifier.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred_svm = svm_classifier.predict(X_test_scaled)

# Evaluate the SVM model
print(classification_report(y_test, y_pred_svm))
print(confusion_matrix(y_test, y_pred_svm))

# 1.9.4 logistic regression

from sklearn.linear_model import LogisticRegression

# Initialize and train a Logistic Regression classifier
logreg_classifier = LogisticRegression(max_iter=1000)
logreg_classifier.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred_logreg = logreg_classifier.predict(X_test_scaled)

# Evaluate the Logistic Regression model
print(classification_report(y_test, y_pred_logreg))
print(confusion_matrix(y_test, y_pred_logreg))

# 1.9.5 random forest

# Initialize and train a Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42) # You can adjust n_estimators
rf_classifier.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred_rf = rf_classifier.predict(X_test_scaled)

# Evaluate the Random Forest model
print(classification_report(y_test, y_pred_rf))
print(confusion_matrix(y_test, y_pred_rf))

# 1.9.6 most important features

# Initialize and train a RandomForestClassifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust parameters
rf_classifier.fit(X_train_scaled, y_train)

# Get feature importances
feature_importances = rf_classifier.feature_importances_

# Print or visualize feature importances
for feature_name, importance in zip(X.columns, feature_importances):
    print(f"{feature_name}: {importance}")

# Visualize feature importances
plt.figure(figsize=(10, 6))
plt.barh(X.columns, feature_importances)
plt.xlabel("Feature Importance")
plt.ylabel("Feature")
plt.title("Feature Importance from RandomForest")
plt.show()

# 1.9.7 neural network

# Initialize and train an MLP classifier
mlp_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42) # Adjust hidden_layer_sizes and max_iter as needed
mlp_classifier.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred_mlp = mlp_classifier.predict(X_test_scaled)

# Evaluate the MLP model
print(classification_report(y_test, y_pred_mlp))
print(confusion_matrix(y_test, y_pred_mlp))

# 1.9.8 decision tree

from sklearn.tree import DecisionTreeClassifier

# Initialize and train a Decision Tree classifier
dt_classifier = DecisionTreeClassifier(random_state=42)  # You can adjust parameters like max_depth, min_samples_split, etc.
dt_classifier.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred_dt = dt_classifier.predict(X_test_scaled)

# Evaluate the Decision Tree model
print(classification_report(y_test, y_pred_dt))
print(confusion_matrix(y_test, y_pred_dt))